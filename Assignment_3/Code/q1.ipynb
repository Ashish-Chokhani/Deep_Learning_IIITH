{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.5746, Eval Loss = 1.4659\n",
      "Epoch 2: Train Loss = 0.7056, Eval Loss = 0.3692\n",
      "Epoch 3: Train Loss = 0.2653, Eval Loss = 0.1119\n",
      "Epoch 4: Train Loss = 0.1325, Eval Loss = 0.0974\n",
      "Epoch 5: Train Loss = 0.1088, Eval Loss = 0.0965\n",
      "Epoch 6: Train Loss = 0.1064, Eval Loss = 0.0961\n",
      "Epoch 7: Train Loss = 0.1037, Eval Loss = 0.0996\n",
      "Epoch 8: Train Loss = 0.0952, Eval Loss = 0.0769\n",
      "Epoch 9: Train Loss = 0.0428, Eval Loss = 0.0044\n",
      "Epoch 10: Train Loss = 0.0118, Eval Loss = 0.0021\n",
      "Obtaining metrics for train data:\n",
      "Eval dataset results:\n",
      "Number of predictions with 0 correct predictions: 0\n",
      "Number of predictions with 1 correct predictions: 0\n",
      "Number of predictions with 2 correct predictions: 0\n",
      "Number of predictions with 3 correct predictions: 0\n",
      "Number of predictions with 4 correct predictions: 59\n",
      "Number of predictions with 5 correct predictions: 3358\n",
      "Number of predictions with 6 correct predictions: 2759\n",
      "Number of predictions with 7 correct predictions: 96\n",
      "Number of predictions with 8 correct predictions: 728\n",
      "Obtaining metrics for eval data:\n",
      "Eval dataset results:\n",
      "Number of predictions with 0 correct predictions: 0\n",
      "Number of predictions with 1 correct predictions: 0\n",
      "Number of predictions with 2 correct predictions: 0\n",
      "Number of predictions with 3 correct predictions: 0\n",
      "Number of predictions with 4 correct predictions: 479\n",
      "Number of predictions with 5 correct predictions: 791\n",
      "Number of predictions with 6 correct predictions: 611\n",
      "Number of predictions with 7 correct predictions: 18\n",
      "Number of predictions with 8 correct predictions: 101\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import os\n",
    "\n",
    "class SentenceTransformDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        data = pd.read_csv(csv_file)\n",
    "        self.sentences = data['Sentence'].apply(lambda x: x).values  \n",
    "        self.transformed_sentences = data['Transformed sentence'].apply(lambda x: '#' + x ).values \n",
    "        self.vocab = self.build_vocab()\n",
    "        self.src_vocab_size = len(self.vocab)\n",
    "        self.idx2char = {i: char for i, char in enumerate(self.vocab)}\n",
    "        self.char2idx = {char: i for i, char in enumerate(self.vocab)}\n",
    "\n",
    "    def build_vocab(self):\n",
    "        sentences = np.concatenate((self.sentences, self.transformed_sentences))\n",
    "        vocab = set(''.join(sentences))\n",
    "        return sorted(list(vocab))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_sentence = self.sentences[idx]\n",
    "        transformed_sentence = self.transformed_sentences[idx]\n",
    "\n",
    "        source_idxs = [self.char2idx[char] for char in source_sentence]\n",
    "        transformed_idxs = [self.char2idx[char] for char in transformed_sentence]\n",
    "\n",
    "        return {\n",
    "            'source': torch.tensor(source_idxs, dtype=torch.long),\n",
    "            'transformed': torch.tensor(transformed_idxs, dtype=torch.long),\n",
    "            'transformed_sentence': transformed_sentence[1:]\n",
    "        }\n",
    "\n",
    "train_dataset = SentenceTransformDataset('train_data.csv')\n",
    "eval_dataset = SentenceTransformDataset('eval_data.csv')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=1)\n",
    "final_train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Transformer Model\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_length = x.size(1)\n",
    "        return x + self.pe[:, :seq_length]\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_output = self.self_attn(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output):\n",
    "        attn_output = self.self_attn(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output\n",
    "\n",
    "# Check if the pre-trained model exists, and if not, train a new model\n",
    "if not torch.cuda.is_available():\n",
    "    device = 'cpu'\n",
    "else:\n",
    "    device = 'cuda'\n",
    "\n",
    "pretrained_model_path = 'transformer_model.pth'\n",
    "\n",
    "if not os.path.exists(pretrained_model_path):\n",
    "    src_vocab_size = 28\n",
    "    tgt_vocab_size = 28\n",
    "    d_model = 256\n",
    "    num_heads = 8\n",
    "    num_layers = 6\n",
    "    d_ff = 2048\n",
    "    max_seq_length = 9\n",
    "    dropout = 0.1\n",
    "\n",
    "    transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "    # Training Loop\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(transformer.parameters(), lr=0.0001)\n",
    "\n",
    "    def train(model, data_loader, criterion, optimizer):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in data_loader:\n",
    "            src_data = batch['source']\n",
    "            tgt_data = batch['transformed']\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src_data,tgt_data[:, :-1])\n",
    "            loss = criterion(output.contiguous().view(-1, tgt_vocab_size),tgt_data[:, 1:].contiguous().view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "    def evaluate(model, data_loader, criterion):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        for batch in data_loader:\n",
    "            src_data = batch['source']\n",
    "            tgt_data = batch['transformed']\n",
    "            output = model(src_data, tgt_data[:, :-1])\n",
    "            loss = criterion(output.contiguous().view(-1, tgt_vocab_size),tgt_data[:, 1:].contiguous().view(-1))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(data_loader)\n",
    "\n",
    "    num_epochs = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(transformer, train_loader, criterion, optimizer)\n",
    "        eval_loss = evaluate(transformer, eval_loader, criterion)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Eval Loss = {eval_loss:.4f}\")\n",
    "\n",
    "    torch.save(transformer.state_dict(), 'transformer_model.pth')\n",
    "\n",
    "else:\n",
    "    # Load the pre-trained model\n",
    "    transformer = Transformer(28, 28, 256, 8, 6, 2048,9, 0.1)\n",
    "    transformer.load_state_dict(torch.load(pretrained_model_path))\n",
    "    transformer.to(device)\n",
    "\n",
    "def check(pred: str, true: str):\n",
    "    correct = 0\n",
    "    for a, b in zip(pred, true):\n",
    "        if a == b:\n",
    "            correct += 1\n",
    "    return correct\n",
    "    \n",
    "def predict_transformed_string(model,idx2char, dataloader,max_seq_length=9):\n",
    "    model.eval()\n",
    "    results = {\n",
    "        \"pred\": [],\n",
    "        \"true\": [],\n",
    "        \"score\": []\n",
    "    }\n",
    "    correct = [0 for _ in range(9)]\n",
    "    cnt=0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            cnt = cnt+1\n",
    "            src = batch['source']\n",
    "            tgt = batch['transformed']\n",
    "            y=batch['transformed_sentence']\n",
    "            y=str(y[0])\n",
    "            output = torch.tensor([train_dataset.char2idx['#']] * max_seq_length, dtype=torch.long).unsqueeze(0)\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            output = torch.argmax(output, dim=-1).tolist()\n",
    "            predicted_string = [idx2char[idx] for idx in output[0]]\n",
    "            final_y_hat=''\n",
    "            for ch in predicted_string:\n",
    "                final_y_hat+=ch\n",
    "            score = check(final_y_hat,y)\n",
    "            results[\"pred\"].append(final_y_hat)\n",
    "            results[\"true\"].append(y)\n",
    "            results[\"score\"].append(score)\n",
    "            correct[score] += 1\n",
    "    print(\"Eval dataset results:\")\n",
    "    for num_chr in range(9):\n",
    "        print(\n",
    "            f\"Number of predictions with {num_chr} correct predictions: {correct[num_chr]}\"\n",
    "        )\n",
    "        \n",
    "print(\"Obtaining metrics for train data:\")\n",
    "predict_transformed_string(transformer,train_dataset.idx2char,final_train_loader)\n",
    "print(\"Obtaining metrics for eval data:\")\n",
    "predict_transformed_string(transformer,train_dataset.idx2char,eval_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
